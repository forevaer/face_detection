{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P_net_Dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        with open(path, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.lines[idx].strip().split()\n",
    "\n",
    "        img_path, label, offset = line[0], line[1], line[2:6]\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label = int(label)\n",
    "        offset = np.array([float(x) for x in offset])\n",
    "        \n",
    "        sample = {'image': img, 'label': label, 'offset': offset}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        image, label, offset = sample.values()\n",
    "        \n",
    "        image = transforms.ToTensor()(image)\n",
    "        label = torch.from_numpy(np.array(label)).float()\n",
    "        offset = torch.from_numpy(offset).float()\n",
    "        \n",
    "        return {'image': image, 'label': label, 'offset': offset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Filp:\n",
    "    def __call__(self, sample):\n",
    "        image, label, offset = sample.values()\n",
    "\n",
    "        pred = random.uniform(0, 1)\n",
    "        if pred > 0.5:\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            offset[0], offset[2] = -offset[2], -offset[0]\n",
    "            \n",
    "        return {'image': image, 'label': label, 'offset': offset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './anno_store/imglist_anno_12.txt'\n",
    "trfm = transforms.Compose([Random_Filp(), ToTensor()])\n",
    "\n",
    "img_show = P_net_Dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20498, 10629, 13756, 8084, 16872]\n",
      "[-1, 0, -1, 0, -1]\n",
      "[array([-0.26, -0.11, -0.13, -0.13]), array([0., 0., 0., 0.]), array([-0.2 , -0.17, -0.14,  0.26]), array([0., 0., 0., 0.]), array([ 0.08, -0.1 ,  0.29,  0.65])]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def my_show(data, nums):\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    lst = []\n",
    "    labels = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i in range(nums):\n",
    "        ax = plt.subplot(1, nums, i + 1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        n = random.randint(0, len(data))\n",
    "        img, label, offset = data[n].values()\n",
    "        \n",
    "        lst.append(n)\n",
    "        labels.append(label)\n",
    "        offsets.append(offset)\n",
    "        \n",
    "        ax.imshow(np.array(img))\n",
    "        ax.axis('off')\n",
    "    print(lst)\n",
    "    print(labels)\n",
    "    print(offsets)\n",
    "    \n",
    "my_show(img_show, 5)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),   # size 10 * 10 * 10\n",
    "            nn.PReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),     # size 5 * 5 * 10\n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),  # size 3 * 3 * 16\n",
    "            nn.PReLU(),  \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),  # size 1 * 1 * 32\n",
    "            nn.PReLU()  \n",
    "        )\n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1) # size 1 * 1 * 1 \n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1) # size 1 * 1 * 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        \n",
    "        # 之后label被用作score\n",
    "        label = torch.sigmoid(self.conv4_1(x))\n",
    "        offset = self.conv4_2(x)\n",
    "\n",
    "        return label, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pnet(model_path, epoch, data_path, trfm, batch_size ,lr):\n",
    "\n",
    "    \n",
    "    dataset = P_net_Dataset(data_path, trfm)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    pnet = PNet()\n",
    "    criterion_cls = nn.BCELoss()\n",
    "    criterion_box = nn.MSELoss()\n",
    "    optimizer = optim.Adam(pnet.parameters(), lr=lr)\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(pnet.state_dict())\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    for epoch_idx in range(epoch):\n",
    "        pnet.train()\n",
    "        epoch_time = time.time()\n",
    "        loss_label_totle = 0.0\n",
    "        loss_offset_totle = 0.0\n",
    "        acc_positive = 0.0\n",
    "        acc_negative = 0.0\n",
    "        times = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            image, label, offset = batch.values()\n",
    "            \n",
    "            # batch的零头数据量过少，可能出现标签都没有的情况\n",
    "            if label.size(0) < batch_size:\n",
    "                continue\n",
    "            \n",
    "            # 进网络\n",
    "            p_label, p_offset = pnet(image)\n",
    "            \n",
    "            # 卷积出来是三维的要压缩掉两维\n",
    "            p_offset = torch.squeeze(p_offset)\n",
    "            p_label = torch.squeeze(p_label)\n",
    "            \n",
    "            # label算loss只要用到1， 0标签的数据\n",
    "            gt_label = label[label != -1]\n",
    "            pre_label = p_label[label != -1]\n",
    "            \n",
    "            # 偏移算loss 用到-1， 1标签的数据（难正样本，正样本）\n",
    "            gt_offset = offset[label != 0]\n",
    "            pre_offset = p_offset[label != 0]\n",
    "\n",
    "            loss_cls = criterion_cls(pre_label, gt_label)\n",
    "            loss_box = criterion_box(pre_offset, gt_offset)\n",
    "            \n",
    "            # 分类loss加了个权重\n",
    "            loss = loss_cls + 0.5 * loss_box\n",
    "            \n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # log\n",
    "            pred = torch.ge(pre_label, 0.6).float()\n",
    "            \n",
    "            acc_positive += (gt_label[gt_label == 1] == pred[gt_label == 1]).sum().item() / (gt_label[gt_label == 1].size(0) + 0.001)\n",
    "            acc_negative += (gt_label[gt_label == 0] == pred[gt_label == 0]).sum().item() / (gt_label[gt_label == 0].size(0) + 0.001)\n",
    "            \n",
    "            loss_label_totle += loss_cls.item()\n",
    "            loss_offset_totle += (loss_box * 0.5).item()\n",
    "            times += 1\n",
    "        \n",
    "        loss_label_totle /= times\n",
    "        loss_offset_totle /= times\n",
    "        loss_totle = loss_label_totle + loss_offset_totle\n",
    "        acc_positive /= times\n",
    "        acc_negative /= times\n",
    "        t = time.time() - epoch_time\n",
    "        \n",
    "        if epoch_idx % 1 == 0:\n",
    "            print('Epoch {} spend: {:.0f}m {:.0f}s'.format(epoch_idx, t // 60, t % 60))\n",
    "            print('Valid: loss_totle: {:.4f} loss_label: {:.4f} loss_offset: {:.4f}'.format(loss_totle, loss_label_totle, loss_offset_totle))\n",
    "            print('Label: acc_positive: {:.4f}, acc_negative: {:.4f}'.format(acc_positive, acc_negative))\n",
    "            print('-' * 20)\n",
    "            \n",
    "        # 取loss最小的参数\n",
    "        if loss_totle < best_valid_loss:\n",
    "            best_model_wts = copy.deepcopy(pnet.state_dict())\n",
    "            best_valid_loss = loss_totle\n",
    "    \n",
    "    pnet.load_state_dict(best_model_wts)\n",
    "    time_elapesd = time.time() - since\n",
    "    torch.save(pnet.state_dict(), model_path)\n",
    "    \n",
    "    print('=' * 50)\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapesd // 60, time_elapesd % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_store/pnet.pkl'\n",
    "data_path = './anno_store/imglist_anno_12.txt'\n",
    "trfm = transforms.Compose([Random_Filp(), ToTensor()])\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 spend: 0m 16s\n",
      "Valid: loss_totle: 0.4285 loss_label: 0.4144 loss_offset: 0.0141\n",
      "Label: acc_positive: 0.0338, acc_negative: 0.9918\n",
      "--------------------\n",
      "Epoch 1 spend: 0m 6s\n",
      "Valid: loss_totle: 0.3525 loss_label: 0.3411 loss_offset: 0.0114\n",
      "Label: acc_positive: 0.3223, acc_negative: 0.9669\n",
      "--------------------\n",
      "Epoch 2 spend: 0m 6s\n",
      "Valid: loss_totle: 0.3035 loss_label: 0.2926 loss_offset: 0.0109\n",
      "Label: acc_positive: 0.5004, acc_negative: 0.9711\n",
      "--------------------\n",
      "Epoch 3 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2634 loss_label: 0.2528 loss_offset: 0.0106\n",
      "Label: acc_positive: 0.6021, acc_negative: 0.9729\n",
      "--------------------\n",
      "Epoch 4 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2411 loss_label: 0.2308 loss_offset: 0.0103\n",
      "Label: acc_positive: 0.6528, acc_negative: 0.9727\n",
      "--------------------\n",
      "Epoch 5 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2290 loss_label: 0.2189 loss_offset: 0.0101\n",
      "Label: acc_positive: 0.6763, acc_negative: 0.9741\n",
      "--------------------\n",
      "Epoch 6 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2204 loss_label: 0.2104 loss_offset: 0.0100\n",
      "Label: acc_positive: 0.6944, acc_negative: 0.9750\n",
      "--------------------\n",
      "Epoch 7 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2110 loss_label: 0.2010 loss_offset: 0.0099\n",
      "Label: acc_positive: 0.7131, acc_negative: 0.9771\n",
      "--------------------\n",
      "Epoch 8 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2086 loss_label: 0.1988 loss_offset: 0.0098\n",
      "Label: acc_positive: 0.7084, acc_negative: 0.9757\n",
      "--------------------\n",
      "Epoch 9 spend: 0m 6s\n",
      "Valid: loss_totle: 0.2035 loss_label: 0.1937 loss_offset: 0.0098\n",
      "Label: acc_positive: 0.7222, acc_negative: 0.9761\n",
      "--------------------\n",
      "Epoch 10 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1956 loss_label: 0.1858 loss_offset: 0.0098\n",
      "Label: acc_positive: 0.7295, acc_negative: 0.9778\n",
      "--------------------\n",
      "Epoch 11 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1853 loss_label: 0.1756 loss_offset: 0.0097\n",
      "Label: acc_positive: 0.7416, acc_negative: 0.9783\n",
      "--------------------\n",
      "Epoch 12 spend: 0m 7s\n",
      "Valid: loss_totle: 0.1885 loss_label: 0.1789 loss_offset: 0.0096\n",
      "Label: acc_positive: 0.7496, acc_negative: 0.9777\n",
      "--------------------\n",
      "Epoch 13 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1798 loss_label: 0.1702 loss_offset: 0.0096\n",
      "Label: acc_positive: 0.7559, acc_negative: 0.9811\n",
      "--------------------\n",
      "Epoch 14 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1816 loss_label: 0.1721 loss_offset: 0.0095\n",
      "Label: acc_positive: 0.7572, acc_negative: 0.9786\n",
      "--------------------\n",
      "Epoch 15 spend: 0m 7s\n",
      "Valid: loss_totle: 0.1736 loss_label: 0.1640 loss_offset: 0.0096\n",
      "Label: acc_positive: 0.7629, acc_negative: 0.9789\n",
      "--------------------\n",
      "Epoch 16 spend: 0m 7s\n",
      "Valid: loss_totle: 0.1673 loss_label: 0.1579 loss_offset: 0.0095\n",
      "Label: acc_positive: 0.7660, acc_negative: 0.9813\n",
      "--------------------\n",
      "Epoch 17 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1681 loss_label: 0.1586 loss_offset: 0.0094\n",
      "Label: acc_positive: 0.7733, acc_negative: 0.9806\n",
      "--------------------\n",
      "Epoch 18 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1670 loss_label: 0.1576 loss_offset: 0.0094\n",
      "Label: acc_positive: 0.7769, acc_negative: 0.9800\n",
      "--------------------\n",
      "Epoch 19 spend: 0m 6s\n",
      "Valid: loss_totle: 0.1661 loss_label: 0.1566 loss_offset: 0.0095\n",
      "Label: acc_positive: 0.7741, acc_negative: 0.9806\n",
      "--------------------\n",
      "==================================================\n",
      "Training complete in 2m 17s\n",
      "Best val loss: 0.166067\n"
     ]
    }
   ],
   "source": [
    "train_pnet(model_path, epoch, data_path, trfm, batch_size ,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
